# IRIS FLOWER SOLUTION PROJECT
## Project:Iris flower classification using Machine Learning
### Machine Learning in python: 
#### Machine learning is is the kind of programming which gives computers the capability to automatically learn from data without being explicitly programmed. 
# IRIS FLOWER CLASSIFACTION
![imagename](https://miro.medium.com/max/361/0*1lgB-Yqej6VPER00)
![imagename](https://miro.medium.com/max/760/0*rhP_m_pskOF_MUad)
### The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper. The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample (in centimetres):
### The aim is to classify iris flowers among three species (setosa, versicolor, or virginica) from measurements of sepals and petals' length and width.
### The iris data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.
### The central goal here is to design a model that makes useful classifications for new flowers or, in other words, one which exhibits good generalization.
### Length of the sepals

### Width of the sepals

### Length of the petals

### Width of the petals.


## This Repository contains the Iris Dataset Project created by using **K-Nearest Neighbor (KNN)**
## K Nearest Neighbors and implementation on Iris data set
![imagename](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/03/knn3-300x271.png)
## This blog focuses on how KNN (K-Nearest Neighbors) algorithm works and implementation of KNN on iris data set and analysis of output.
### 1.knn explanation
### K-Nearest Neighbors is one of the most basic yet essential classification algorithms in Machine Learning. It belongs to the supervised learning domain and finds intense application in pattern recognition, data mining.
### KNN algorithm can also be used for regression problems.The only difference will be using averages of nearest neighbors rather than voting from nearest neighbors.
### KNN algorithm makes predictions by calculating similarity between the input sample and each training instance. This algorithm does not make strong assumptions about the form of mapping function hence it is Nonparametric. In simple words, by not making assumptions, the algorithm is free to learn any functional form from the training data.
### In K-NN algorithm output is a class membership.An object is assigned a class which is most common among its K nearest neighbors,K being the number of neighbors.Intuitively K is always a positive integer.
## KNN can be summarized as below:
### 1.Computes the distance between the new data point with every training example.
### 2.For computing the distance measures such as Euclidean distance, Hamming distance or Manhattan distance will be used.
### 3.Model picks K entries in the database which are closest to the new data point.
### 4.Then it does the majority vote i.e the most common class/label among those K entries will be the class of the new data point.
##  By using the Knn technique we have found the accuracy 
